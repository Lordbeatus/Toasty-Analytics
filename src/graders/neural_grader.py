"""
Neural Network Based Grader
Uses deep learning for code quality assessment
"""

import os
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
from src.core.base_grader import BaseGrader, GraderResult
from src.core.types import GradingDimension, ImprovementSuggestion, ScoreBreakdown

# Optional ML dependencies
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset
    from transformers import AutoModel, AutoTokenizer

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    print("âš ï¸  PyTorch not available - Neural grader will use fallback mode")


if TORCH_AVAILABLE:

    class CodeEmbedding(nn.Module):
        """
        Neural network that creates embeddings from code
        Uses CodeBERT or similar pre-trained models
        """

        def __init__(self, model_name: str = "microsoft/codebert-base"):
            super().__init__()
            if not TORCH_AVAILABLE:
                raise ImportError("PyTorch required for neural grading")

            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.encoder = AutoModel.from_pretrained(model_name)

            # Quality prediction head
            self.quality_head = nn.Sequential(
                nn.Linear(768, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, 64),
                nn.ReLU(),
                nn.Linear(64, 1),
                nn.Sigmoid(),  # Output 0-1 for quality score
            )

        def forward(self, input_ids, attention_mask):
            """Forward pass through the network"""
            # Get code embeddings from encoder
            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)

            # Use [CLS] token embedding
            cls_embedding = outputs.last_hidden_state[:, 0, :]

            # Predict quality score
            quality_score = self.quality_head(cls_embedding)

            return quality_score

else:
    CodeEmbedding = None


class NeuralGrader(BaseGrader):
    """
    Neural network-based grader using deep learning
    Trained on historical grading data
    """

    dimension = GradingDimension.CODE_QUALITY

    def __init__(self, model_path: Optional[str] = None):
        super().__init__()
        self.model = None
        self.tokenizer = None
        self.device = (
            torch.device("cuda" if torch.cuda.is_available() else "cpu")
            if TORCH_AVAILABLE
            else None
        )

        if TORCH_AVAILABLE:
            self.load_model(model_path)
        else:
            print("âš ï¸  Neural grader in fallback mode - will use rule-based scoring")

    def load_model(self, model_path: Optional[str] = None):
        """Load pre-trained model"""
        if not TORCH_AVAILABLE:
            return

        if model_path and Path(model_path).exists():
            # Load fine-tuned model
            self.model = torch.load(model_path, map_location=self.device)
            self.model.eval()
            print(f"âœ… Loaded neural grader model from {model_path}")
        else:
            # Initialize with pre-trained CodeBERT (no fine-tuning yet)
            try:
                self.model = CodeEmbedding()
                self.tokenizer = self.model.tokenizer
                self.model.to(self.device)
                self.model.eval()
                print("âœ… Initialized neural grader with CodeBERT (needs training)")
            except Exception as e:
                print(f"âš ï¸  Failed to load CodeBERT: {e}")
                self.model = None

    def grade(self, code: str, language: str = "python", **context) -> GraderResult:
        """Grade code using neural network"""

        if not TORCH_AVAILABLE or self.model is None:
            # Fallback to simple rule-based scoring
            return self._fallback_grade(code, language)

        try:
            # Tokenize code
            inputs = self.tokenizer(
                code,
                max_length=512,
                padding="max_length",
                truncation=True,
                return_tensors="pt",
            )

            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Get prediction
            with torch.no_grad():
                quality_score = self.model(**inputs)

            # Convert to 0-100 scale
            score = float(quality_score.item() * 100)

            # Generate feedback based on score
            if score >= 90:
                feedback = "ðŸŒŸ Excellent code quality! Neural model rates this highly."
            elif score >= 75:
                feedback = "âœ… Good code quality detected by neural analysis."
            elif score >= 60:
                feedback = "âš ï¸  Moderate quality - consider improvements."
            else:
                feedback = "âŒ Low quality code detected by neural analysis."

            suggestions = self._generate_suggestions(score)

            return GraderResult(
                dimension=self.dimension,
                score=score,
                max_score=100,
                breakdown=ScoreBreakdown(
                    dimension=self.dimension,
                    score=score,
                    max_score=100,
                    weight=1.0,
                    weighted_score=score,
                    rationale="Neural network assessment using CodeBERT embeddings",
                    line_level_feedback={},
                    suggestions=[s.description for s in suggestions],
                ),
                feedback=feedback,
                suggestions=suggestions,
                metadata={
                    "model": "CodeBERT",
                    "device": str(self.device),
                    "confidence": float(
                        abs(quality_score.item() - 0.5) * 2
                    ),  # 0-1 confidence
                },
            )

        except Exception as e:
            print(f"âš ï¸  Neural grading failed: {e}")
            return self._fallback_grade(code, language)

    def _fallback_grade(self, code: str, language: str) -> GraderResult:
        """Simple rule-based fallback when neural model unavailable"""
        score = 70  # Default moderate score

        # Simple heuristics
        lines = code.split("\n")
        non_empty_lines = [l for l in lines if l.strip()]

        # Length heuristic
        if len(non_empty_lines) > 100:
            score -= 5

        # Comment heuristic
        comment_lines = [l for l in lines if l.strip().startswith("#")]
        comment_ratio = len(comment_lines) / max(1, len(non_empty_lines))
        if comment_ratio > 0.1:
            score += 10

        # Function definition heuristic
        if "def " in code:
            score += 5

        # Class definition heuristic
        if "class " in code:
            score += 5

        score = max(0, min(100, score))

        return GraderResult(
            dimension=self.dimension,
            score=score,
            max_score=100,
            breakdown=ScoreBreakdown(
                dimension=self.dimension,
                score=score,
                max_score=100,
                weight=1.0,
                weighted_score=score,
                rationale="Fallback rule-based scoring (neural model unavailable)",
                line_level_feedback={},
                suggestions=[],
            ),
            feedback=f"Fallback scoring: {score}/100 (install PyTorch for neural grading)",
            suggestions=[],
            metadata={"mode": "fallback"},
        )

    def _generate_suggestions(self, score: float) -> List[ImprovementSuggestion]:
        """Generate suggestions based on score"""
        suggestions = []

        if score < 90:
            suggestions.append(
                ImprovementSuggestion(
                    category="code_quality",
                    description="Consider refactoring for better code structure",
                    priority="medium",
                    example="Break down large functions into smaller, focused ones",
                )
            )

        if score < 75:
            suggestions.append(
                ImprovementSuggestion(
                    category="readability",
                    description="Improve code documentation and naming",
                    priority="high",
                    example="Add docstrings and use descriptive variable names",
                )
            )

        if score < 60:
            suggestions.append(
                ImprovementSuggestion(
                    category="best_practices",
                    description="Follow Python best practices and style guidelines",
                    priority="high",
                    example="Use PEP 8 style guide and type hints",
                )
            )

        return suggestions

    def train(
        self,
        training_data: List[Dict[str, Any]],
        epochs: int = 10,
        batch_size: int = 16,
    ):
        """
        Train the neural model on historical grading data

        Args:
            training_data: List of {"code": str, "score": float, "language": str}
            epochs: Number of training epochs
            batch_size: Batch size for training
        """
        if not TORCH_AVAILABLE or self.model is None:
            print("âŒ Cannot train - PyTorch or model not available")
            return

        print(f"ðŸ§  Training neural grader on {len(training_data)} samples...")

        # TODO: Implement training loop
        # This is a placeholder for the full training implementation

        print("âš ï¸  Training not fully implemented yet - this is a placeholder")
        print("   Next steps:")
        print("   1. Create custom Dataset class")
        print("   2. Implement training loop with optimizer")
        print("   3. Add validation split")
        print("   4. Implement early stopping")
        print("   5. Save best model checkpoint")
